{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0286798",
   "metadata": {},
   "source": [
    "# Welcome!\n",
    "This notebook intends to be a template for an end-to-end (or, perhaps more aptly, a discovery-analysis-publish) pipeline for using Open NASA data. In this version, there is much more documentation/tutorial, as we explain _what_ is supposed to be happening and _why_ we have done these things.\n",
    "\n",
    "### Bookend Structure\n",
    "![bookend-structure](./figures/bookend-structure.png)\n",
    "\n",
    "### Knowledge Graphs & Semantic Technologies\n",
    "* [What is Metadata](https://github.com/KGConf/open-kg-curriculum/blob/master/curriculum/modules/What_is_Metadata/What_is_Metadata.md)\n",
    "* [What is an Identifier](https://github.com/KGConf/open-kg-curriculum/blob/master/curriculum/modules/What_is_an_Identifier/What_is_an_Identifier.md)\n",
    "* [What is a KG](https://github.com/KGConf/open-kg-curriculum/blob/master/curriculum/modules/What_is_a_Knowledge_Graph/What_is_a_Knowledge_Graph.md)\n",
    "* [What is a Taxonomy](https://github.com/KGConf/open-kg-curriculum/blob/master/curriculum/modules/What_is_a_Taxonomy/What_is_a_Taxonomy.md)\n",
    "* [What is an Ontology](https://github.com/KGConf/open-kg-curriculum/blob/master/curriculum/modules/What_is_an_Ontology/What_is_an_Ontology.md)\n",
    "\n",
    "#### Ontology Design Patterns\n",
    "Ontology Design Patterns (ODPs) are self-contained miniature ontologies that solve domain-invariant modeling problems. Our approach uses several to create a modular \"plug and play\" KG schema (or architecture).\n",
    "* Computational Environment\n",
    "* [Computational Observation](https://github.com/kastle-lab/computational-observation-pattern)\n",
    "* [Data Transformation](https://github.com/Data-Semantics-Laboratory/data-transformation-pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa71a07",
   "metadata": {},
   "source": [
    "## Bookend Software\n",
    "* [rdflib](https://rdflib.readthedocs.io/en/stable/)\n",
    "* [sparqlwrapper](https://sparqlwrapper.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6cf6e8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdflib is the general purpose python library for modifying a kg in memory and outputting it to a file\n",
    "import rdflib\n",
    "## Just some convenient classes to pull out\n",
    "from rdflib import URIRef, Graph, Namespace, Literal\n",
    "## namespaces are below. These are where identifiers \"live\", so to speak.\n",
    "from rdflib import OWL, RDF, RDFS, XSD, TIME\n",
    "\n",
    "# sparqlwrapper is used to query a triplestore\n",
    "import SPARQLWrapper\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6216aa76",
   "metadata": {},
   "source": [
    "## Prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4c80e5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some default prefixes for namespaces.\n",
    "# Which are generally useful\n",
    "pfs = {\n",
    "\"geo\": Namespace(\"http://www.opengis.net/ont/geosparql#\"),\n",
    "\"geof\": Namespace(\"http://www.opengis.net/def/function/geosparql/\"),\n",
    "\"sf\": Namespace(\"http://www.opengis.net/ont/sf#\"),\n",
    "\"wd\": Namespace(\"http://www.wikidata.org/entity/\"),\n",
    "\"wdt\": Namespace(\"http://www.wikidata.org/prop/direct/\"),\n",
    "\"dbo\": Namespace(\"http://dbpedia.org/ontology/\"),\n",
    "\"time\": Namespace(\"http://www.w3.org/2006/time#\"),\n",
    "\"ssn\": Namespace(\"http://www.w3.org/ns/ssn/\"),\n",
    "\"sosa\": Namespace(\"http://www.w3.org/ns/sosa/\"),\n",
    "\"cdt\": Namespace(\"http://w3id.org/lindt/custom_datatypes#\"),\n",
    "\"ex\": Namespace(\"https://example.com/\"),\n",
    "\"rdf\": RDF,\n",
    "\"rdfs\": RDFS,\n",
    "\"xsd\": XSD,\n",
    "\"owl\": OWL,\n",
    "\"time\": TIME\n",
    "}\n",
    "\n",
    "# The namespace and prefixes which we will use for the metadata storage\n",
    "name_space = \"https://polyneme.xyz/\"\n",
    "pfs[\"polyr\"] = Namespace(f\"{name_space}lod/resource#\")\n",
    "pfs[\"poly-ont\"] =  Namespace(f\"{name_space}lod/ontology#\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b57280",
   "metadata": {},
   "source": [
    "## Storing Metadata\n",
    "It should perhaps come as no surprise the rest of the notebook, but we will store the metadata generated in this notebook in a knowledge graph. For now, it will stay in memory as `Graph` from `rdflib`. When we publish the dataset generated in this notebook, we will upload the dataset into a graph database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9afa019d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_kg(prefixes=pfs):\n",
    "    kg = Graph()\n",
    "    for prefix in pfs:\n",
    "        kg.bind(prefix, pfs[prefix])\n",
    "    return kg\n",
    "# rdf:type shortcut\n",
    "a = pfs[\"rdf\"][\"type\"]\n",
    "\n",
    "# Initialize an empty graph\n",
    "graph = init_kg()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb294f2",
   "metadata": {},
   "source": [
    "## Accessing Your Local Graph Database\n",
    "For this notebook, we assume you are running a `developer` (i.e., non-production) Apache Jena Fuseki triplestore as your graph database. This will be useful in several different cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "09b7e98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection success.\n"
     ]
    }
   ],
   "source": [
    "db_loc = \"http://localhost:3030\"\n",
    "dataset = \"bookend\"\n",
    "endpoint = f\"{db_loc}/{dataset}\"\n",
    "sparql = SPARQLWrapper(endpoint) # default location of fuseki\n",
    "sparql.setReturnFormat(JSON)\n",
    "\n",
    "# Construct the query\n",
    "query = \"\"\"\n",
    "    PREFIX poly-ont: <https://polyneme.xyz/lod/ontology#>\n",
    "\n",
    "    SELECT *\n",
    "    WHERE {\n",
    "        ?person a poly-ont:Person .\n",
    "    }\n",
    "    \"\"\"\n",
    "# Set the query\n",
    "sparql.setQuery(query)\n",
    "\n",
    "try:\n",
    "    ret = sparql.queryAndConvert()\n",
    "    print(\"Connection success.\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "06777a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This query will be useful\n",
    "def instanceExists(uri, sparql=sparql):\n",
    "    query = f\"\"\"\n",
    "    PREFIX poly-ont: <https://polyneme.xyz/lod/ontology#>\n",
    "\n",
    "    SELECT *\n",
    "    WHERE {{\n",
    "        <{uri}> ?p ?o .\n",
    "    }}\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    "    sparql.setQuery(query)\n",
    "    try:\n",
    "        ret = sparql.queryAndConvert()\n",
    "        return len(ret[\"results\"][\"bindings\"]) == 1\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e00634",
   "metadata": {},
   "source": [
    "## Capturing the Current Computational Environment\n",
    "![computational environment](./figures/computational-environment-pattern.png)\n",
    "The purpose of this is to capture the environment in which you transform data (i.e., create something new from something old). This is useful for replicability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e1ed503e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to populate this pattern for this notebook\n",
    "\n",
    "## Mint a URI for this computational environment\n",
    "### There are many ways to create an identifier\n",
    "### We have chosen a way that encodes some information for identifiability, without searching for the label.\n",
    "comp_env_name = \"polyneme.donny.home\"\n",
    "#### Some other examples might be\n",
    "#### \"polyneme.cogan.wsu\"\n",
    "#### \"organization.name.location\"\n",
    "comp_env_uri = pfs[\"polyr\"][comp_env_name]\n",
    "## Check to see if the computational environment exists\n",
    "if instanceExists(comp_env_uri):\n",
    "    ## Check if there needs to be updates\n",
    "    pass\n",
    "    ## Otherwise, moveon\n",
    "    pass\n",
    "else:\n",
    "    ## If you have done this before (i.e., this is not your first time running this notebook) AND your \n",
    "    ## computational environment hasn't changed.\n",
    "    graph.add( (comp_env_uri, a, pfs[\"poly-ont\"][\"ComputationalEnvironment\"]) )\n",
    "    ### TODO read comp_env from config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b7cc5d",
   "metadata": {},
   "source": [
    "## Computational Observations\n",
    "![simulation activity](./figures/computational-observation-pattern.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "356f6c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to populate this pattern for this notebook\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0241607",
   "metadata": {},
   "source": [
    "## Dataset Discovery\n",
    "This is effectively the \"Extract\" part of the ETL loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "77a30b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to do data set discovery\n",
    "## PySat?\n",
    "## HDPE.io\n",
    "## CDAWeb\n",
    "pass\n",
    "\n",
    "# It is important to note that we have a record of which datasets we start with\n",
    "data_inputs = list()\n",
    "\n",
    "## Populated sample dataset names\n",
    "data_inputs = [\"dataset1\", \"dataset2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c7aece",
   "metadata": {},
   "source": [
    "## This is where your code goes!\n",
    "This is effectively the \"Transform\" part of the ETL loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6bb4c8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "bce85dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO support multiple output datasets from a single script\n",
    "# It is important to provide the name and location of where the data exists\n",
    "# Optionally, you can give the format (e.g., csv)\n",
    "result_name = \"example.data\"\n",
    "result_loc  = \"https://example.org/sample/data/loc\"\n",
    "result_format = \"csv\"\n",
    "## Uniqueness is necessary!\n",
    "output_uri = pfs[\"polyr\"][f\"Data.{result_name}\"]\n",
    "if instanceExists(output_uri):\n",
    "    print(\"You have chosen a result name which exists in the graph database already.\")\n",
    "    print(\"If you continue, you will accidentally merge any metadata for this with any previously committed version.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ef1513",
   "metadata": {},
   "source": [
    "## Dataset Publishing (Internal)\n",
    "Now it is time to publish your work. This is effectively the \"Load\" part of the ETL loop.\n",
    "\n",
    "### Data Transformation Pattern\n",
    "![data transformation pattern](./figures/data-transformation-pattern.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9cb7965a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling\n",
    "## Output Data\n",
    "output_uri = pfs[\"polyr\"][f\"Data.{result_name}\"]\n",
    "graph.add((output_uri, a, pfs[\"poly-ont\"][\"Data\"])) # declaration\n",
    "### Associate the dataset with its actual data (i.e., the payload)\n",
    "#### This reification exists in case we want to have other metadata about the location link (e.g., access permissions)\n",
    "payload_uri = pfs[\"polyr\"][f\"Payload.{result_name}\"]\n",
    "graph.add((payload_uri, a, pfs[\"poly-ont\"][\"Payload\"])) # declaration\n",
    "loc_uri = URIRef(result_loc)\n",
    "graph.add((output_uri, pfs[\"poly-ont\"][\"hasPayload\"], payload_uri)) # association\n",
    "graph.add((payload_uri, pfs[\"poly-ont\"][\"hasLocation\"], loc_uri)) # association\n",
    "\n",
    "### Associate the datatype to the dataset\n",
    "format_uri = pfs[\"polyr\"][f\"DataType.{result_format}\"]\n",
    "graph.add( (output_uri, pfs[\"poly-ont\"][\"hasDataType\"], format_uri) )\n",
    "\n",
    "## DataTransformation\n",
    "\n",
    "### TODO global notion of uniqueness for dt\n",
    "dt_id = 1\n",
    "dt_uri = pfs[\"polyr\"][f\"DataTransformation.{dt_id}\"]\n",
    "graph.add( (dt_uri, a, pfs[\"poly-ont\"][\"DataTransformation\"]) )\n",
    "graph.add( (dt_uri, pfs[\"poly-ont\"][\"occursInCE\"], comp_env_uri))\n",
    "### TODO spatiotemporal extent (or at least temporal extent)\n",
    "### TODO association of the DataTransformation with ComputationalModelExecution\n",
    "\n",
    "## Input Data & Data Roles\n",
    "for data_input in data_inputs:\n",
    "    input_uri = pfs[\"polyr\"][f\"Data.{data_input}\"]\n",
    "    graph.add((input_uri, a, pfs[\"poly-ont\"][\"Data\"])) # declaration\n",
    "    \n",
    "    ## Mint a new input data role\n",
    "    input_role_uri = pfs[\"polyr\"][f\"InputRole.DT{dt_id}.{data_input}\"]\n",
    "    graph.add((input_role_uri, a, pfs[\"poly-ont\"][\"InputRole\"] )) # declaration\n",
    "    graph.add((input_uri, pfs[\"poly-ont\"][\"performsInputRole\"], input_role_uri)) # association\n",
    "    graph.add((dt_uri, pfs[\"poly-ont\"][\"providesInputRole\"], input_role_uri)) # association\n",
    "\n",
    "## Output Data Role\n",
    "output_role_uri = pfs[\"polyr\"][f\"OutputRole.DT{dt_id}.{result_name}\"]\n",
    "graph.add((output_role_uri, a, pfs[\"poly-ont\"][\"InputRole\"] )) # declaration\n",
    "graph.add((output_uri, pfs[\"poly-ont\"][\"performsOutputRole\"], output_role_uri)) # association\n",
    "graph.add((dt_uri, pfs[\"poly-ont\"][\"providesOutputRole\"], output_role_uri)) # association\n",
    "pass # prevents cell output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6cf822fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'g' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [119], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m output_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput.ttl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m temp \u001b[38;5;241m=\u001b[39m \u001b[43mg\u001b[49m\u001b[38;5;241m.\u001b[39mserialize(\u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mturtle\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m, destination\u001b[38;5;241m=\u001b[39moutput_file)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'g' is not defined"
     ]
    }
   ],
   "source": [
    "output_file = \"output.ttl\"\n",
    "temp = g.serialize(format=\"turtle\", encoding=\"utf-8\", destination=output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "360c8e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push to graph database\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
